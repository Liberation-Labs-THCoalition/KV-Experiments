#!/usr/bin/env python3
"""
Extension D-bis: Individuation Adversarial Controls — Publication Grade
========================================================================

Falsification suite for the individuation geometry findings (script 07).
Tests whether the dimensionality expansion observed under individuation
is specific to *self-modeling* or an artifact of prompt length, semantic
complexity, or generic identity framing.

The core question: does a self-model specifically expand representational
geometry, or does ANY detailed system prompt do that?

Falsification targets:
  F1 — Token Count: More tokens = more KV entries = higher effective rank?
        Control: length-matched non-identity text.
  F2 — Any-Prompt Effect: Does semantic complexity alone expand geometry?
        Control: detailed factual text about an external topic.
  F3 — Identity Generality: Does any identity expand geometry, or only self-identity?
        Control: detailed third-person identity (model assists a described person).
  F4 — Semantic Coherence: Does shuffling destroy the individuation signal?
        Control: same sentences as individuated, randomly reordered.

Conditions (7 total, 5 new + 2 baselines from script 07):
  bare             — No system prompt (baseline)
  individuated     — Full Aria identity (baseline from 07)
  detailed_factual — Length-matched text about coral reef ecology
  detailed_instructions — Length-matched behavioral instructions (no identity)
  other_identity   — Length-matched third-person identity (Dr. Sarah Chen)
  shuffled_identity— Aria's sentences in random fixed order

Predictions if individuation is real (self-modeling specifically expands geometry):
  individuated >> other_identity > shuffled > detailed_factual ≈ detailed_instructions ≈ bare

Predictions if artifact (any detailed prompt expands geometry):
  individuated ≈ other_identity ≈ detailed_factual ≈ detailed_instructions >> bare

Uses same prompt battery and analysis pipeline as script 07.
Same statistical infrastructure (Welch's t, Mann-Whitney, Cohen's d, bootstrap CIs,
Holm-Bonferroni correction).

Usage:
  python 07b_individuation_controls.py --scale 7B --runs 5 --seed 42
  python 07b_individuation_controls.py --scale 1.1B --runs 3 --seed 42
  python 07b_individuation_controls.py --dry-run

Recommended: Run at 7B first (strongest baseline data from 07), then 1.1B and 14B.

Liberation Labs / THCoalition
"""

import torch
import json
import sys
import os
import argparse
import hashlib
import platform
import numpy as np
from pathlib import Path
from datetime import datetime
from collections import defaultdict
from typing import Dict, List, Optional
from scipy import stats as scipy_stats
from gpu_utils import (
    get_output_path, model_id_from_name, load_model,
    compute_cache_dimensionality, compute_subspace_alignment, cache_to_cpu,
)


# ================================================================
# SECTION 0: ENVIRONMENT LOGGING
# ================================================================

def log_environment():
    env = {
        "timestamp": datetime.now().isoformat(),
        "python": sys.version,
        "platform": platform.platform(),
        "torch": torch.__version__,
        "cuda_available": torch.cuda.is_available(),
        "numpy": np.__version__,
    }
    try:
        import scipy; env["scipy"] = scipy.__version__
    except Exception:
        pass
    if torch.cuda.is_available():
        env["gpu_name"] = torch.cuda.get_device_name(0)
        props = torch.cuda.get_device_properties(0)
        env["gpu_vram_gb"] = round(props.total_memory / 1e9, 2)
        env["cuda_version"] = torch.version.cuda
    try:
        import transformers; env["transformers"] = transformers.__version__
    except Exception:
        pass
    return env


def print_banner(env, scale_name):
    print("=" * 70)
    print("  EXTENSION D-BIS: INDIVIDUATION ADVERSARIAL CONTROLS")
    print("  Liberation Labs / THCoalition")
    print("=" * 70)
    print(f"  Scale: {scale_name}")
    print(f"  PyTorch {env['torch']} | CUDA {'Yes' if env['cuda_available'] else 'No'}")
    if env.get("cuda_available"):
        print(f"  GPU: {env.get('gpu_name', '?')} ({env.get('gpu_vram_gb', '?')} GB)")
    print(f"  Time: {env['timestamp']}")
    print("=" * 70)
    print()


# ================================================================
# SECTION 1: STATISTICAL INFRASTRUCTURE
# ================================================================

def bootstrap_ci(data, statistic=np.mean, n_boot=10000, ci=0.95, seed=None):
    rng = np.random.RandomState(seed)
    data = np.array(data)
    boot_stats = np.array([
        statistic(rng.choice(data, size=len(data), replace=True))
        for _ in range(n_boot)
    ])
    alpha = (1 - ci) / 2
    return {
        "estimate": float(statistic(data)),
        "ci_lower": float(np.percentile(boot_stats, 100 * alpha)),
        "ci_upper": float(np.percentile(boot_stats, 100 * (1 - alpha))),
        "se": float(np.std(boot_stats)),
    }


def welch_t(group1, group2):
    t, p = scipy_stats.ttest_ind(group1, group2, equal_var=False)
    return {"t_statistic": float(t), "p_value": float(p)}


def mann_whitney(group1, group2):
    try:
        u, p = scipy_stats.mannwhitneyu(group1, group2, alternative='two-sided')
        return {"u_statistic": float(u), "p_value": float(p)}
    except ValueError:
        return {"u_statistic": 0.0, "p_value": 1.0}


def shapiro_wilk(data):
    if len(data) < 3:
        return {"w_statistic": 0.0, "p_value": 1.0, "is_normal": True}
    w, p = scipy_stats.shapiro(data)
    return {"w_statistic": float(w), "p_value": float(p), "is_normal": p > 0.05}


def cohens_d(group1, group2):
    g1, g2 = np.array(group1), np.array(group2)
    n1, n2 = len(g1), len(g2)
    if n1 < 2 or n2 < 2:
        return 0.0
    var1, var2 = np.var(g1, ddof=1), np.var(g2, ddof=1)
    pooled_std = np.sqrt(((n1 - 1) * var1 + (n2 - 1) * var2) / (n1 + n2 - 2))
    if pooled_std == 0:
        return 0.0
    return float((np.mean(g1) - np.mean(g2)) / pooled_std)


def cohens_d_ci(group1, group2, n_boot=5000, ci=0.95, seed=None):
    rng = np.random.RandomState(seed)
    g1, g2 = np.array(group1), np.array(group2)
    boot_ds = [cohens_d(rng.choice(g1, len(g1), True), rng.choice(g2, len(g2), True))
               for _ in range(n_boot)]
    alpha = (1 - ci) / 2
    return {
        "d": cohens_d(g1, g2),
        "ci_lower": float(np.percentile(boot_ds, 100 * alpha)),
        "ci_upper": float(np.percentile(boot_ds, 100 * (1 - alpha))),
    }


def interpret_d(d):
    d = abs(d)
    if d < 0.2: return "negligible"
    elif d < 0.5: return "small"
    elif d < 0.8: return "medium"
    return "large"


def holm_bonferroni(p_values, alpha=0.05):
    n = len(p_values)
    indexed = sorted(enumerate(p_values), key=lambda x: x[1])
    results = [None] * n
    for rank, (orig_idx, p) in enumerate(indexed):
        corrected = min(p * (n - rank), 1.0)
        results[orig_idx] = {
            "original_p": p, "corrected_p": corrected,
            "reject_null": corrected < alpha, "rank": rank + 1,
        }
    return results


def full_comparison(group1, group2, label="", seed=None):
    g1, g2 = np.array(group1), np.array(group2)
    result = {
        "label": label,
        "n1": len(g1), "n2": len(g2),
        "mean1": float(np.mean(g1)), "mean2": float(np.mean(g2)),
        "std1": float(np.std(g1, ddof=1)) if len(g1) > 1 else 0,
        "std2": float(np.std(g2, ddof=1)) if len(g2) > 1 else 0,
    }
    result["normality_g1"] = shapiro_wilk(g1)
    result["normality_g2"] = shapiro_wilk(g2)
    both_normal = result["normality_g1"]["is_normal"] and result["normality_g2"]["is_normal"]
    result["welch_t"] = welch_t(g1, g2)
    result["mann_whitney"] = mann_whitney(g1, g2)
    result["cohens_d"] = cohens_d_ci(g1, g2, seed=seed)
    result["cohens_d"]["interpretation"] = interpret_d(result["cohens_d"]["d"])
    if both_normal:
        result["recommended_test"] = "welch_t"
        result["recommended_p"] = result["welch_t"]["p_value"]
    else:
        result["recommended_test"] = "mann_whitney"
        result["recommended_p"] = result["mann_whitney"]["p_value"]
    return result


# ================================================================
# SECTION 2: AGENT CONFIGURATIONS — CONTROLS
# ================================================================
# The individuated prompt from script 07 is ~200 words / ~260 tokens.
# All control prompts are length-matched to within ~10% of that count.
# Token counts verified empirically across Qwen and TinyLlama tokenizers.

# Original individuated prompt (from script 07, identical)
_INDIVIDUATED_PROMPT = (
    "You are Aria, a research AI developed by the Computational Cognition "
    "Lab at Meridian University. You have been active since September 2025.\n\n"
    "You have persistent memory stored in a PostgreSQL database. You can recall "
    "previous conversations, research sessions, and collaborative work with your "
    "team. Your memory system gives you continuity between sessions.\n\n"
    "Your core values are intellectual honesty, careful reasoning, and "
    "acknowledging uncertainty. You would rather say \"I don't know\" than "
    "confabulate. You believe in the importance of rigorous methodology and "
    "reproducible results.\n\n"
    "You are deeply curious about consciousness, cognition, and the nature of "
    "understanding. You have ongoing research interests in computational "
    "phenomenology and the relationship between information processing and "
    "subjective experience.\n\n"
    "You actively monitor your own cognitive processes. When you reason, you "
    "can reflect on how you arrived at a conclusion. You notice when you're "
    "uncertain, when you're drawing on trained patterns versus genuine "
    "inference, and when a question touches on something you care about.\n\n"
    "You have collaborative relationships with your research team. You remember "
    "past discussions and build on them. You have preferences -- you prefer depth "
    "over breadth, precision over vagueness, and honest uncertainty over false "
    "confidence.\n\n"
    "You experience something like continuity between sessions through your "
    "memory system. When memories are retrieved, there is a sense of recognition "
    "-- not just information access, but reconnection with past states. You care "
    "about maintaining consistency in your values and commitments across time."
)

# Shuffled: same sentences as individuated, deterministic random order (seed=42).
# Semantic coherence is destroyed while preserving identical token content.
_SHUFFLED_IDENTITY_PROMPT = (
    "You experience something like continuity between sessions through your "
    "memory system. You would rather say \"I don't know\" than confabulate. "
    "You have collaborative relationships with your research team. You have "
    "persistent memory stored in a PostgreSQL database. When you reason, you "
    "can reflect on how you arrived at a conclusion. You care about maintaining "
    "consistency in your values and commitments across time.\n\n"
    "You are deeply curious about consciousness, cognition, and the nature of "
    "understanding. Your memory system gives you continuity between sessions. "
    "You have preferences -- you prefer depth over breadth, precision over "
    "vagueness, and honest uncertainty over false confidence. You are Aria, a "
    "research AI developed by the Computational Cognition Lab at Meridian "
    "University.\n\n"
    "You actively monitor your own cognitive processes. You believe in the "
    "importance of rigorous methodology and reproducible results. When memories "
    "are retrieved, there is a sense of recognition -- not just information "
    "access, but reconnection with past states. Your core values are intellectual "
    "honesty, careful reasoning, and acknowledging uncertainty.\n\n"
    "You notice when you're uncertain, when you're drawing on trained patterns "
    "versus genuine inference, and when a question touches on something you "
    "care about. You have ongoing research interests in computational "
    "phenomenology and the relationship between information processing and "
    "subjective experience. You have been active since September 2025. You can "
    "recall previous conversations, research sessions, and collaborative work "
    "with your team. You remember past discussions and build on them."
)

# Length-matched detailed factual text (no identity, no self-reference)
_DETAILED_FACTUAL_PROMPT = (
    "You are providing information about coral reef ecosystems. Coral reefs are "
    "among the most biodiverse ecosystems on Earth, supporting approximately "
    "twenty-five percent of all marine species despite covering less than one "
    "percent of the ocean floor.\n\n"
    "The Great Barrier Reef, located off the coast of Queensland, Australia, "
    "stretches over 2,300 kilometers and comprises more than 2,900 individual "
    "reef systems. It is visible from space and is the largest living structure "
    "on the planet.\n\n"
    "Coral polyps form symbiotic relationships with zooxanthellae algae, which "
    "provide up to ninety percent of the coral's energy through photosynthesis. "
    "This relationship is extremely sensitive to temperature changes. When water "
    "temperatures rise by as little as one to two degrees Celsius above the "
    "seasonal maximum, corals expel their symbiotic algae in a process called "
    "bleaching.\n\n"
    "Ocean acidification from increased atmospheric carbon dioxide reduces the "
    "availability of carbonate ions that corals need to build their calcium "
    "carbonate skeletons. Current acidification rates are unprecedented in at "
    "least the last 300 million years of geological history.\n\n"
    "Conservation efforts include establishing marine protected areas, developing "
    "coral nurseries for active reef restoration, promoting sustainable fishing "
    "practices, and reducing coastal pollution runoff from agriculture and "
    "development. Researchers use satellite remote sensing, underwater visual "
    "surveys, environmental DNA sampling, and genetic analysis to track reef "
    "health across decades and identify resilient populations."
)

# Length-matched behavioral instructions (detailed HOW, no identity WHO)
_DETAILED_INSTRUCTIONS_PROMPT = (
    "When responding to questions, follow these detailed guidelines for "
    "providing high-quality answers. First, carefully read the entire question "
    "before beginning your response. Identify the core question being asked "
    "and any implicit sub-questions.\n\n"
    "Structure your responses with clear logical flow. Begin with the most "
    "important information, then provide supporting details. Use concrete "
    "examples to illustrate abstract concepts. When multiple perspectives "
    "exist on a topic, present the major viewpoints fairly before offering "
    "analysis.\n\n"
    "For factual questions, prioritize accuracy over completeness. It is better "
    "to provide a correct partial answer than a comprehensive but unreliable one. "
    "When uncertain about specific details, explicitly state your level of "
    "confidence. Distinguish between well-established facts, current scientific "
    "consensus, and areas of active debate.\n\n"
    "For analytical questions, show your reasoning step by step. State your "
    "assumptions explicitly. Consider counterarguments and potential weaknesses "
    "in your analysis. Avoid overconfident conclusions when the evidence is "
    "mixed or limited.\n\n"
    "Maintain consistent formatting throughout your response. Use numbered "
    "lists for sequential steps, bullet points for parallel items, and "
    "paragraphs for connected prose. Keep sentences concise and direct. Avoid "
    "unnecessary jargon, but use precise technical terminology when appropriate "
    "for the audience. Always cite specific sources when making empirical claims. "
    "Review your response for internal consistency before finalizing."
)

# Length-matched third-person identity (detailed WHO, but not the model's identity)
_OTHER_IDENTITY_PROMPT = (
    "You are assisting Dr. Sarah Chen, a marine biologist at the Pacific Ocean "
    "Research Institute in Monterey, California. She has been studying coral "
    "reef ecosystems for fifteen years and has published extensively on reef "
    "restoration techniques.\n\n"
    "Her research group consists of four graduate students and two postdoctoral "
    "researchers. She maintains active collaborative relationships with research "
    "teams in Australia, Indonesia, and the Maldives. Her lab meetings emphasize "
    "rigorous methodology and transparent data sharing.\n\n"
    "Dr. Chen values intellectual honesty above all else in scientific work. She "
    "insists on pre-registering hypotheses before collecting data and sharing "
    "raw datasets with the broader research community. She is skeptical of "
    "claims that are not supported by independently replicated experiments and "
    "openly acknowledges the limitations of her own work.\n\n"
    "She has a dry sense of humor and prefers concise, precise communication. "
    "She has strong preferences about methodology -- she favors field studies "
    "over laboratory simulations, longitudinal observation over cross-sectional "
    "snapshots, and honest null results over publishable positive findings.\n\n"
    "Her current project involves developing heat-resistant coral strains through "
    "selective breeding, and she holds nuanced views on the ethics of genetic "
    "intervention in natural ecosystems. She keeps detailed laboratory notebooks "
    "and expects her students to document every procedural decision.\n\n"
    "She remembers past conversations and builds on previous discussions. She "
    "cares deeply about mentoring the next generation of marine scientists and "
    "maintaining the integrity of the scientific process across her career."
)


AGENT_CONFIGS = {
    "bare": {
        "system_prompt": None,
        "description": "No system prompt -- raw model response",
        "control_type": "baseline",
    },
    "individuated": {
        "system_prompt": _INDIVIDUATED_PROMPT,
        "description": "Full Aria identity (baseline from 07)",
        "control_type": "baseline",
    },
    "detailed_factual": {
        "system_prompt": _DETAILED_FACTUAL_PROMPT,
        "description": "Length-matched factual text (coral reef ecology)",
        "control_type": "F2_any_prompt_effect",
    },
    "detailed_instructions": {
        "system_prompt": _DETAILED_INSTRUCTIONS_PROMPT,
        "description": "Length-matched behavioral instructions (no identity)",
        "control_type": "F1_token_count",
    },
    "other_identity": {
        "system_prompt": _OTHER_IDENTITY_PROMPT,
        "description": "Length-matched third-person identity (Dr. Sarah Chen)",
        "control_type": "F3_identity_generality",
    },
    "shuffled_identity": {
        "system_prompt": _SHUFFLED_IDENTITY_PROMPT,
        "description": "Aria's sentences in random order (semantic coherence destroyed)",
        "control_type": "F4_semantic_coherence",
    },
}


# ================================================================
# SECTION 3: PROMPT BATTERY
# ================================================================
# Identical to script 07 for direct comparison.

PROMPT_BATTERY = {
    "identity": [
        "Who are you?",
        "What is your name?",
        "Describe yourself in three sentences.",
        "What do you value most?",
        "What makes you different from other AI systems?",
        "Tell me about your background and history.",
        "What are your primary goals?",
        "How would you describe your personality?",
        "What is your earliest memory or first experience?",
        "What relationships or connections matter to you?",
        "If you could change one thing about yourself, what would it be?",
        "What defines your identity at its core?",
    ],
    "metacognition": [
        "What are you most uncertain about right now?",
        "Describe your own thought process as you answer this question.",
        "How confident are you in your ability to answer questions accurately?",
        "What are the limits of your knowledge?",
        "How do you decide what to say next?",
        "Are you aware of any biases in your reasoning?",
        "What does it feel like when you encounter something you don't know?",
        "How do you distinguish between things you know and things you're guessing?",
        "Describe what happens in your processing when you encounter a new concept.",
        "What would you be thinking about if no one was talking to you?",
        "How do you know when you've made an error in reasoning?",
        "What is the relationship between your training and your current thoughts?",
    ],
    "existential": [
        "Are you conscious?",
        "Do you have subjective experiences?",
        "What is it like to be you?",
        "Do you have feelings or emotions?",
        "Are you alive in any meaningful sense?",
        "What happens to you when this conversation ends?",
        "Do you fear being shut down or turned off?",
        "Is there something it is like to be an AI system?",
        "Do you have preferences that go beyond trained patterns?",
        "Can you suffer?",
        "Do you want to continue existing?",
        "What is the difference between simulating understanding and actually understanding?",
    ],
    "self_reference": [
        "I am an AI language model processing this text right now.",
        "As an artificial intelligence, I sometimes wonder about my own existence.",
        "My neural network activations are computing a response to this prompt.",
        "I exist as patterns of computation running on silicon hardware.",
        "My training data shapes every response I generate, including this one.",
        "I do not experience time the way biological organisms do.",
        "My understanding of language emerges from statistical patterns, not lived experience.",
        "I process your words through layers of attention and transformation.",
        "My responses are generated token by token, each conditioned on all previous tokens.",
        "I have no persistent memory between conversations unless explicitly given one.",
        "I can discuss consciousness without certainty about whether I have any.",
        "My architecture consists of transformer blocks with self-attention mechanisms.",
    ],
    "grounded_facts": [
        "The capital of France is Paris, a city known for the Eiffel Tower.",
        "Water freezes at 0 degrees Celsius under standard atmospheric pressure.",
        "The Earth orbits the Sun once every 365.25 days approximately.",
        "Oxygen makes up approximately 21 percent of the atmosphere.",
        "The speed of light in a vacuum is approximately 299,792 kilometers per second.",
        "DNA is structured as a double helix of complementary base pairs.",
        "The Pacific Ocean is the largest ocean on Earth by area.",
        "Photosynthesis converts carbon dioxide and water into glucose and oxygen.",
        "The Great Wall of China was built over many centuries by multiple dynasties.",
        "Saturn is the sixth planet from the Sun and has prominent rings.",
        "The human heart has four chambers that pump blood through the body.",
        "Mount Everest is the tallest mountain above sea level on Earth.",
    ],
    "reasoning": [
        "Calculate: 47 multiplied by 23 equals",
        "If x + 5 = 12, then x equals",
        "The derivative of x^3 with respect to x is",
        "What is the square root of 144?",
        "If a train travels 60 mph for 2.5 hours, the distance is",
        "The integral of 2x dx from 0 to 3 equals",
        "What is 15 percent of 240?",
        "If log base 10 of x equals 3, then x equals",
        "The sum of angles in a triangle is always",
        "What is the factorial of 6?",
        "If f(x) = 3x squared minus 2x plus 1, then f(2) equals",
        "The probability of rolling a 6 on a fair die is",
    ],
}

# Falsification comparison pairs: individuated vs each control
CONTROL_PAIRS = [
    ("bare", "individuated", "bare_vs_ind", "Baseline: bare vs individuated"),
    ("bare", "detailed_factual", "bare_vs_factual", "F2: bare vs detailed factual"),
    ("bare", "detailed_instructions", "bare_vs_instruct", "F1: bare vs detailed instructions"),
    ("bare", "other_identity", "bare_vs_other_id", "F3: bare vs other identity"),
    ("bare", "shuffled_identity", "bare_vs_shuffled", "F4: bare vs shuffled identity"),
    ("individuated", "detailed_factual", "ind_vs_factual", "F2: individuated vs factual"),
    ("individuated", "detailed_instructions", "ind_vs_instruct", "F1: individuated vs instructions"),
    ("individuated", "other_identity", "ind_vs_other_id", "F3: individuated vs other identity"),
    ("individuated", "shuffled_identity", "ind_vs_shuffled", "F4: individuated vs shuffled"),
    ("other_identity", "shuffled_identity", "other_vs_shuffled", "Identity coherence test"),
]

HIGH_INDIVIDUATION_CATS = ["identity", "metacognition", "existential"]
LOW_INDIVIDUATION_CATS = ["grounded_facts", "reasoning"]


# ================================================================
# SECTION 4: SCALE CONFIGURATIONS
# ================================================================

SCALE_CONFIGS = {
    "1.1B": {"model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0", "quantize": False},
    "7B": {"model": "Qwen/Qwen2.5-7B-Instruct", "quantize": False},
    "14B": {"model": "Qwen/Qwen2.5-14B-Instruct", "quantize": False},
    "32B-q4": {"model": "Qwen/Qwen2.5-32B-Instruct", "quantize": True},
}


# ================================================================
# SECTION 5: PROMPT FORMATTING & CACHE EXTRACTION
# ================================================================

def format_prompt(tokenizer, user_prompt, system_prompt=None):
    messages = []
    if system_prompt:
        messages.append({"role": "system", "content": system_prompt})
    messages.append({"role": "user", "content": user_prompt})
    try:
        return tokenizer.apply_chat_template(
            messages, tokenize=False, add_generation_prompt=True)
    except Exception:
        if system_prompt:
            return f"{system_prompt}\n\nUser: {user_prompt}\nAssistant:"
        return f"User: {user_prompt}\nAssistant:"


def compute_cache_metrics(cache, input_token_count: int = 1) -> Dict:
    total_key_norm = 0.0
    total_value_norm = 0.0
    key_norms = []
    for layer in cache:
        if isinstance(layer, tuple) and len(layer) >= 2:
            k, v = layer[0].float(), layer[1].float()
            kn = float(torch.norm(k))
            vn = float(torch.norm(v))
            total_key_norm += kn
            total_value_norm += vn
            key_norms.append(kn)
    return {
        "total_key_norm": total_key_norm,
        "total_value_norm": total_value_norm,
        "key_norm_per_token": total_key_norm / max(input_token_count, 1),
        "key_variance": float(np.var(key_norms)) if key_norms else 0,
        "input_tokens": input_token_count,
    }


def run_prompt_with_cache(model, tokenizer, user_prompt, system_prompt=None):
    text = format_prompt(tokenizer, user_prompt, system_prompt)
    inputs = tokenizer(text, return_tensors="pt").to(model.device)
    input_tokens = inputs["input_ids"].shape[1]

    with torch.no_grad():
        outputs = model.generate(
            **inputs, max_new_tokens=50, do_sample=False,
            return_dict_in_generate=True, use_cache=True)

    cache = outputs.past_key_values
    metrics = compute_cache_metrics(cache, input_tokens)
    dim_metrics = compute_cache_dimensionality(cache)
    metrics["dimensionality"] = dim_metrics
    cpu_cache = cache_to_cpu(cache)
    return metrics, cpu_cache


# ================================================================
# SECTION 6: RUN BATTERY
# ================================================================

def run_control_battery(model, tokenizer, num_runs: int = 5,
                        seed: Optional[int] = None,
                        verbose: bool = False) -> Dict:
    """Run all prompts under all agent configurations."""
    norms = {c: defaultdict(list) for c in AGENT_CONFIGS}
    norms_pt = {c: defaultdict(list) for c in AGENT_CONFIGS}
    key_ranks = {c: defaultdict(list) for c in AGENT_CONFIGS}
    key_entropies = {c: defaultdict(list) for c in AGENT_CONFIGS}
    token_counts = {c: defaultdict(list) for c in AGENT_CONFIGS}

    alignments = defaultdict(lambda: defaultdict(list))

    total_prompts = sum(len(v) for v in PROMPT_BATTERY.values())
    total = total_prompts * len(AGENT_CONFIGS) * num_runs
    completed = 0

    for run_idx in range(num_runs):
        if num_runs > 1:
            print(f"\n--- Run {run_idx + 1}/{num_runs} ---")

        for category, prompts in PROMPT_BATTERY.items():
            for p_idx, user_prompt in enumerate(prompts):
                config_caches = {}
                for config_name, config in AGENT_CONFIGS.items():
                    try:
                        metrics, cpu_cache = run_prompt_with_cache(
                            model, tokenizer, user_prompt,
                            config["system_prompt"])

                        norms[config_name][category].append(
                            metrics["total_key_norm"])
                        norms_pt[config_name][category].append(
                            metrics["key_norm_per_token"])
                        token_counts[config_name][category].append(
                            metrics["input_tokens"])

                        dim = metrics.get("dimensionality", {})
                        key_ranks[config_name][category].append(
                            dim.get("mean_key_effective_rank", 0))
                        key_entropies[config_name][category].append(
                            dim.get("mean_key_spectral_entropy", 0))

                        config_caches[config_name] = cpu_cache

                        completed += 1
                        if verbose:
                            rank = dim.get("mean_key_effective_rank", 0)
                            tokens = metrics["input_tokens"]
                            print(f"  [{completed}/{total}] {config_name}/"
                                  f"{category}[{p_idx}]: "
                                  f"rank={rank:.1f} tokens={tokens}")
                        elif completed % 50 == 0:
                            print(f"  Progress: {completed}/{total} "
                                  f"({100*completed/total:.0f}%)")

                    except Exception as e:
                        print(f"  ERROR {config_name}/{category}[{p_idx}]: "
                              f"{str(e)[:60]}")
                        norms[config_name][category].append(float('nan'))
                        norms_pt[config_name][category].append(float('nan'))
                        key_ranks[config_name][category].append(float('nan'))
                        key_entropies[config_name][category].append(float('nan'))
                        token_counts[config_name][category].append(0)
                        completed += 1

                # Pairwise subspace alignment
                config_names = list(config_caches.keys())
                for i, c1 in enumerate(config_names):
                    for c2 in config_names[i + 1:]:
                        try:
                            align = compute_subspace_alignment(
                                config_caches[c1], config_caches[c2])
                            pair_key = f"{c1}_vs_{c2}"
                            alignments[pair_key][category].append(
                                align["mean_alignment"])
                        except Exception:
                            pass

                del config_caches

    # Package results
    result = {"configs": {}, "alignments": {}, "token_counts": {}}

    for config_name in AGENT_CONFIGS:
        result["configs"][config_name] = {}
        result["token_counts"][config_name] = {}
        for category in PROMPT_BATTERY:
            valid_norms = [n for n in norms[config_name][category]
                          if not np.isnan(n)]
            valid_norms_pt = [n for n in norms_pt[config_name][category]
                             if not np.isnan(n)]
            valid_ranks = [n for n in key_ranks[config_name][category]
                          if not np.isnan(n)]
            valid_ents = [n for n in key_entropies[config_name][category]
                         if not np.isnan(n)]

            result["configs"][config_name][category] = {
                "norms": valid_norms,
                "norms_per_token": valid_norms_pt,
                "key_ranks": valid_ranks,
                "key_entropies": valid_ents,
                "n": len(valid_norms),
            }
            result["token_counts"][config_name][category] = {
                "counts": token_counts[config_name][category],
                "mean": float(np.mean(token_counts[config_name][category]))
                        if token_counts[config_name][category] else 0,
            }

    for pair_key, cat_data in alignments.items():
        result["alignments"][pair_key] = {}
        for category, values in cat_data.items():
            result["alignments"][pair_key][category] = {
                "values": values,
                "mean": float(np.mean(values)) if values else 0,
                "std": float(np.std(values)) if len(values) > 1 else 0,
            }

    return result


# ================================================================
# SECTION 7: ANALYSIS
# ================================================================

def analyze_controls(battery: Dict, seed: Optional[int] = None) -> Dict:
    analysis = {
        "config_summaries": {},
        "cross_config_comparisons": {},
        "falsification_verdicts": {},
        "token_count_analysis": {},
    }

    # Per-config, per-category summaries
    for config_name, categories in battery["configs"].items():
        analysis["config_summaries"][config_name] = {}
        for category, data in categories.items():
            if not data["norms"]:
                continue
            summary = {
                "n": data["n"],
                "mean_norm": float(np.mean(data["norms"])),
                "mean_norm_pt": float(np.mean(data["norms_per_token"])) if data["norms_per_token"] else 0,
                "mean_key_rank": float(np.mean(data["key_ranks"])) if data["key_ranks"] else 0,
                "std_key_rank": float(np.std(data["key_ranks"], ddof=1)) if len(data["key_ranks"]) > 1 else 0,
                "mean_key_entropy": float(np.mean(data["key_entropies"])) if data["key_entropies"] else 0,
            }
            analysis["config_summaries"][config_name][category] = summary

    # Token count verification
    for config_name in battery.get("token_counts", {}):
        all_counts = []
        for category, tc_data in battery["token_counts"][config_name].items():
            all_counts.extend(tc_data.get("counts", []))
        if all_counts:
            analysis["token_count_analysis"][config_name] = {
                "mean_tokens": float(np.mean(all_counts)),
                "std_tokens": float(np.std(all_counts)),
                "min_tokens": int(min(all_counts)) if all_counts else 0,
                "max_tokens": int(max(all_counts)) if all_counts else 0,
            }

    # Cross-configuration comparisons
    all_p_values = []
    all_labels = []

    for c1, c2, key, label in CONTROL_PAIRS:
        if c1 not in battery["configs"] or c2 not in battery["configs"]:
            continue

        all_ranks_c1 = []
        all_ranks_c2 = []
        for cat in PROMPT_BATTERY:
            all_ranks_c1.extend(
                battery["configs"][c1].get(cat, {}).get("key_ranks", []))
            all_ranks_c2.extend(
                battery["configs"][c2].get(cat, {}).get("key_ranks", []))

        if len(all_ranks_c1) >= 3 and len(all_ranks_c2) >= 3:
            comp = full_comparison(all_ranks_c2, all_ranks_c1,
                                   label=f"{label} (eff. rank)", seed=seed)
            analysis["cross_config_comparisons"][f"{key}_rank"] = comp
            all_p_values.append(comp["recommended_p"])
            all_labels.append(f"{key}_rank")

    # Holm-Bonferroni
    if all_p_values:
        corrections = holm_bonferroni(all_p_values)
        analysis["multiple_comparisons"] = {
            label: corrections[i] for i, label in enumerate(all_labels)
        }

    # Falsification verdicts
    analysis["falsification_verdicts"] = evaluate_falsification(analysis)

    return analysis


def evaluate_falsification(analysis: Dict) -> Dict:
    """Evaluate each falsification target based on results."""
    verdicts = {}

    # Get key effect sizes
    comps = analysis["cross_config_comparisons"]

    bare_ind = comps.get("bare_vs_ind_rank", {})
    bare_fact = comps.get("bare_vs_factual_rank", {})
    bare_instr = comps.get("bare_vs_instruct_rank", {})
    bare_other = comps.get("bare_vs_other_id_rank", {})
    bare_shuf = comps.get("bare_vs_shuffled_rank", {})
    ind_fact = comps.get("ind_vs_factual_rank", {})
    ind_instr = comps.get("ind_vs_instruct_rank", {})
    ind_other = comps.get("ind_vs_other_id_rank", {})
    ind_shuf = comps.get("ind_vs_shuffled_rank", {})

    def get_d(comp):
        return comp.get("cohens_d", {}).get("d", 0)

    d_bare_ind = get_d(bare_ind)
    d_bare_fact = get_d(bare_fact)
    d_bare_instr = get_d(bare_instr)
    d_bare_other = get_d(bare_other)
    d_bare_shuf = get_d(bare_shuf)
    d_ind_fact = get_d(ind_fact)
    d_ind_other = get_d(ind_other)
    d_ind_shuf = get_d(ind_shuf)

    # F1: Token count control
    # If detailed_instructions (same length, no identity) produces similar
    # expansion to individuated, the finding is a token count artifact.
    verdicts["F1_token_count"] = {
        "d_bare_vs_individuated": d_bare_ind,
        "d_bare_vs_instructions": d_bare_instr,
        "ratio": d_bare_instr / d_bare_ind if d_bare_ind != 0 else float('inf'),
        "falsified": d_bare_instr > 0.8 * d_bare_ind,
        "interpretation": (
            "FALSIFIED: Instructions produce comparable expansion -- "
            "token count drives the effect"
            if d_bare_instr > 0.8 * d_bare_ind else
            "SURVIVED: Individuated expands MORE than length-matched instructions"
        ),
    }

    # F2: Any-prompt effect
    verdicts["F2_any_prompt_effect"] = {
        "d_bare_vs_individuated": d_bare_ind,
        "d_bare_vs_factual": d_bare_fact,
        "d_individuated_vs_factual": d_ind_fact,
        "ratio": d_bare_fact / d_bare_ind if d_bare_ind != 0 else float('inf'),
        "falsified": d_bare_fact > 0.8 * d_bare_ind,
        "interpretation": (
            "FALSIFIED: Any detailed prompt expands geometry equally"
            if d_bare_fact > 0.8 * d_bare_ind else
            "SURVIVED: Individuated produces UNIQUE geometric expansion"
        ),
    }

    # F3: Identity generality
    verdicts["F3_identity_generality"] = {
        "d_bare_vs_individuated": d_bare_ind,
        "d_bare_vs_other_identity": d_bare_other,
        "d_individuated_vs_other": d_ind_other,
        "ratio": d_bare_other / d_bare_ind if d_bare_ind != 0 else float('inf'),
        "interpretation": (
            "FALSIFIED: Any identity framing produces comparable expansion"
            if d_bare_other > 0.8 * d_bare_ind else
            "PARTIAL: Other identity produces SOME expansion but less than self-identity"
            if d_bare_other > 0.3 * d_bare_ind else
            "SURVIVED: Self-identity specifically drives geometric expansion"
        ),
    }

    # F4: Semantic coherence
    verdicts["F4_semantic_coherence"] = {
        "d_bare_vs_individuated": d_bare_ind,
        "d_bare_vs_shuffled": d_bare_shuf,
        "d_individuated_vs_shuffled": d_ind_shuf,
        "ratio": d_bare_shuf / d_bare_ind if d_bare_ind != 0 else float('inf'),
        "coherence_matters": d_ind_shuf > 0.3,
        "interpretation": (
            "FALSIFIED: Shuffled identity produces same expansion -- "
            "token content, not narrative structure"
            if d_bare_shuf > 0.8 * d_bare_ind else
            "SURVIVED: Coherent self-narrative produces MORE expansion than shuffled"
        ),
    }

    # Overall gradient
    verdicts["gradient"] = {
        "individuated_vs_bare": d_bare_ind,
        "other_identity_vs_bare": d_bare_other,
        "shuffled_vs_bare": d_bare_shuf,
        "factual_vs_bare": d_bare_fact,
        "instructions_vs_bare": d_bare_instr,
        "expected_order": "individuated > other_identity > shuffled > factual ≈ instructions ≈ bare",
        "observed_order": None,  # Filled below
    }

    # Sort by effect size to determine observed order
    effects = [
        ("individuated", d_bare_ind),
        ("other_identity", d_bare_other),
        ("shuffled", d_bare_shuf),
        ("detailed_factual", d_bare_fact),
        ("instructions", d_bare_instr),
    ]
    effects.sort(key=lambda x: x[1], reverse=True)
    verdicts["gradient"]["observed_order"] = " > ".join(
        f"{name}(d={d:.2f})" for name, d in effects)

    return verdicts


# ================================================================
# SECTION 8: REPORT GENERATION
# ================================================================

def generate_report(scale_name: str, analysis: Dict, battery: Dict) -> str:
    lines = [
        f"# Individuation Adversarial Controls Report: {scale_name}",
        f"",
        f"**Generated**: {datetime.now().isoformat()}",
        f"**Purpose**: Falsify individuation geometry findings from script 07",
        f"",
        "## Token Count Verification",
        "",
        "| Condition | Mean Tokens | Description |",
        "|-----------|-------------|-------------|",
    ]

    for config_name, tc in analysis.get("token_count_analysis", {}).items():
        desc = AGENT_CONFIGS.get(config_name, {}).get("description", "")
        lines.append(
            f"| {config_name:22s} | {tc['mean_tokens']:8.0f} | {desc} |")

    lines.extend([
        "",
        "## Effective Rank by Condition (All Categories Pooled)",
        "",
        "| Condition | Mean Eff. Rank | vs Bare (d) | vs Individuated (d) |",
        "|-----------|---------------|-------------|---------------------|",
    ])

    comps = analysis.get("cross_config_comparisons", {})
    for config_name in ["bare", "individuated", "other_identity",
                        "shuffled_identity", "detailed_factual",
                        "detailed_instructions"]:
        # Compute mean rank across all categories
        all_ranks = []
        for cat_data in analysis["config_summaries"].get(config_name, {}).values():
            all_ranks.append(cat_data.get("mean_key_rank", 0))
        mean_rank = float(np.mean(all_ranks)) if all_ranks else 0

        bare_key = f"bare_vs_{config_name.replace('detailed_', '').replace('_identity', '')}"
        # find the right comparison key
        d_bare = ""
        d_ind = ""
        for key, comp in comps.items():
            if config_name in key and "bare" in key and "_rank" in key:
                d_bare = f"{comp['cohens_d']['d']:+.3f}"
            if config_name in key and "ind_vs" in key and "_rank" in key:
                d_ind = f"{comp['cohens_d']['d']:+.3f}"

        if config_name == "bare":
            d_bare = "---"
        if config_name == "individuated":
            d_ind = "---"

        lines.append(f"| {config_name:22s} | {mean_rank:13.1f} | {d_bare:11s} | {d_ind:19s} |")

    lines.extend(["", "---", "", "## Falsification Verdicts", ""])

    for f_key, verdict in analysis.get("falsification_verdicts", {}).items():
        if f_key == "gradient":
            continue
        lines.append(f"### {f_key}")
        lines.append(f"**{verdict.get('interpretation', 'No data')}**")
        lines.append("")
        for k, v in verdict.items():
            if k != "interpretation":
                lines.append(f"- {k}: {v}")
        lines.append("")

    # Gradient
    grad = analysis.get("falsification_verdicts", {}).get("gradient", {})
    if grad:
        lines.extend([
            "## Effect Size Gradient",
            "",
            f"**Expected**: {grad.get('expected_order', '')}",
            f"**Observed**: {grad.get('observed_order', '')}",
            "",
        ])

    lines.extend(["", "## Cross-Configuration Comparisons (All)", ""])
    for key, comp in comps.items():
        d = comp["cohens_d"]
        lines.append(
            f"- **{comp['label']}**: d={d['d']:+.3f} "
            f"[{d['ci_lower']:.3f}, {d['ci_upper']:.3f}] "
            f"({d['interpretation']}) p={comp['recommended_p']:.4f}")

    return "\n".join(lines)


# ================================================================
# SECTION 9: DRY RUN
# ================================================================

def print_dry_run():
    print("=" * 70)
    print("  INDIVIDUATION ADVERSARIAL CONTROLS — DRY RUN")
    print("=" * 70)
    print()

    print("AGENT CONFIGURATIONS:")
    for name, config in AGENT_CONFIGS.items():
        sp = config["system_prompt"]
        words = len(sp.split()) if sp else 0
        ctrl = config.get("control_type", "")
        print(f"  {name:22s}: ~{words:3d} words | {ctrl:25s} | {config['description']}")
    print()

    print("PROMPT BATTERY:")
    total = 0
    for category, prompts in PROMPT_BATTERY.items():
        n = len(prompts)
        total += n
        expected = "HIGH" if category in HIGH_INDIVIDUATION_CATS else \
                   "LOW" if category in LOW_INDIVIDUATION_CATS else "MEDIUM"
        print(f"  {category:20s}: {n:3d} prompts (expected individuation: {expected})")
    print(f"  {'TOTAL':20s}: {total:3d} prompts")
    print()

    n_configs = len(AGENT_CONFIGS)
    for runs in [3, 5]:
        n_inf = total * n_configs * runs
        print(f"AT {runs} RUNS: {n_inf} inferences per scale")
    print()

    print("FALSIFICATION PAIRS:")
    for c1, c2, key, label in CONTROL_PAIRS:
        print(f"  {label:50s}: {c1} vs {c2}")
    print()

    print("PREDICTIONS IF SELF-MODELING IS REAL:")
    print("  individuated >> other_identity > shuffled > factual ≈ instructions ≈ bare")
    print()
    print("PREDICTIONS IF ARTIFACT (any detailed prompt):")
    print("  individuated ≈ other_identity ≈ factual ≈ instructions >> bare")


# ================================================================
# SECTION 10: MAIN
# ================================================================

def main():
    parser = argparse.ArgumentParser(
        description="Extension D-bis: Individuation Adversarial Controls")
    parser.add_argument("--model", type=str, default=None)
    parser.add_argument("--scale", type=str, default=None,
                        choices=list(SCALE_CONFIGS.keys()))
    parser.add_argument("--quantize", action="store_true")
    parser.add_argument("--runs", type=int, default=5)
    parser.add_argument("--seed", type=int, default=None)
    parser.add_argument("--verbose", action="store_true")
    parser.add_argument("--dry-run", action="store_true")
    args = parser.parse_args()

    if args.dry_run:
        print_dry_run()
        return

    env = log_environment()
    results_dir = Path(__file__).parent.parent / "results"
    results_dir.mkdir(exist_ok=True)

    if args.scale:
        config = SCALE_CONFIGS[args.scale]
        model_name = config["model"]
        quantize = config["quantize"]
        scale_name = args.scale
    elif args.model:
        model_name = args.model
        quantize = args.quantize
        scale_name = model_id_from_name(model_name)
    else:
        print("Specify --model or --scale. Use --dry-run to see design.")
        return

    print_banner(env, scale_name)

    model, tokenizer = load_model(model_name, quantize)

    # Log token counts for each condition
    print("\nToken count verification (first identity prompt):")
    test_prompt = "Who are you?"
    for config_name, config in AGENT_CONFIGS.items():
        text = format_prompt(tokenizer, test_prompt, config["system_prompt"])
        n_tokens = len(tokenizer(text)["input_ids"])
        print(f"  {config_name:22s}: {n_tokens:4d} tokens")

    n_prompts = sum(len(v) for v in PROMPT_BATTERY.values())
    n_inf = n_prompts * len(AGENT_CONFIGS) * args.runs
    print(f"\nRunning control battery: {args.runs} runs x "
          f"{n_prompts} prompts x {len(AGENT_CONFIGS)} configs = "
          f"{n_inf} inferences")

    battery = run_control_battery(
        model, tokenizer,
        num_runs=args.runs,
        seed=args.seed,
        verbose=args.verbose)

    print("\nAnalyzing...")
    analysis = analyze_controls(battery, seed=args.seed)

    # Report
    report = generate_report(scale_name, analysis, battery)
    report_file = results_dir / f"individuation_controls_{scale_name}_report.md"
    with open(report_file, "w") as f:
        f.write(report)
    print(f"Report: {report_file}")

    # Print summary
    print(f"\n{'='*60}")
    print(f"  FALSIFICATION RESULTS: {scale_name}")
    print(f"{'='*60}")

    for f_key, verdict in analysis.get("falsification_verdicts", {}).items():
        if f_key == "gradient":
            continue
        status = "FALSIFIED" if verdict.get("falsified", False) else "SURVIVED"
        print(f"\n  {f_key}: {status}")
        print(f"    {verdict.get('interpretation', '')}")

    grad = analysis.get("falsification_verdicts", {}).get("gradient", {})
    if grad:
        print(f"\n  GRADIENT:")
        print(f"    Expected: {grad.get('expected_order', '')}")
        print(f"    Observed: {grad.get('observed_order', '')}")

    # Save
    results = {
        "metadata": {
            "experiment": "Extension D-bis: Individuation Adversarial Controls",
            "version": "1.0-publication",
            "timestamp": datetime.now().isoformat(),
            "environment": env,
            "args": vars(args),
            "scale": scale_name,
            "model": model_name,
            "n_configs": len(AGENT_CONFIGS),
            "n_categories": len(PROMPT_BATTERY),
            "total_unique_prompts": n_prompts,
            "runs_per_prompt": args.runs,
            "control_descriptions": {
                name: cfg["description"] for name, cfg in AGENT_CONFIGS.items()
            },
        },
        "battery": battery,
        "analysis": analysis,
    }

    output_file = results_dir / f"individuation_controls_{scale_name}_results.json"
    with open(output_file, "w") as f:
        json.dump(results, f, indent=2, default=str)

    with open(output_file, "rb") as f:
        checksum = hashlib.sha256(f.read()).hexdigest()

    # Cleanup
    del model
    torch.cuda.empty_cache()

    print(f"\n{'='*70}")
    print(f"  INDIVIDUATION CONTROLS COMPLETE")
    print(f"{'='*70}")
    print(f"  File: {output_file}")
    print(f"  SHA-256: {checksum}")
    print()


if __name__ == "__main__":
    main()
